{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Requirements Document (PRD): CHF-Automator\n",
    "\n",
    "## 1. Executive Summary\n",
    "**Project Name:** `CHF-Automator`  \n",
    "**Objective:** Automate the \"Crop Health Factor (CHF)\" workflow (Murthy et al., 2022) for paddy crop insurance using Google Earth Engine (GEE).  \n",
    "**Core Philosophy:** \n",
    "1. **Decoupled Architecture:** Heavy GEE extraction is separated from mathematical modeling. Data is persisted to local CSVs in between steps to prevent timeouts.\n",
    "2. **Dynamic Masking:** Every year uses its own specific Crop Map (`N+1` strategy).\n",
    "3. **Training vs. Application:** Weights are learned from Historic variance but applied to All years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Technical Stack\n",
    "* **Platform:** Google Earth Engine (Python API) for extraction.\n",
    "* **Environment:** Python 3.9+ (Pandas/NumPy) for modeling.\n",
    "* **Authentication:** Modular GEE Project initialization (handled externally)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Input Specifications\n",
    "The tool requires the following inputs via a configuration file or main script:\n",
    "\n",
    "1.  **Insurance Units (Vector):** GEE Asset ID (User provided) containing `Unit_ID` (Unique) and `Strata_ID` (Grouping).\n",
    "2.  **Season Dates:** `season_start`, `season_end`, `peak_start`, `peak_end`.\n",
    "3.  **Analysis Configuration:**\n",
    "    * **Years List:** A list of all years to process (e.g., `[2018, 2019, 2020, 2021, 2022, 2023]`).\n",
    "    * **Crop Map Dictionary:** A mapping of `Year -> Crop_Map_Asset_ID`.\n",
    "    * **Training Years:** A subset list (e.g., `[2018, 2019, 2020, 2021, 2022]`) used strictly for weight generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The 8 Indicators (GEE Processing Logic)\n",
    "**Constraint:** This logic applies to *every* year independently, using that year's specific Crop Mask.\n",
    "\n",
    "| Indicator | Data Source | GEE Processing Logic |\n",
    "| :--- | :--- | :--- |\n",
    "| **1. Max NDVI** | Sentinel-2 | **Cloud Mask:** Use `MSK_CLDPRB`. Mask pixels where `MSK_CLDPRB > 20`. <br>**Reducer:** `.max()` over `season_start` to `season_end`. |\n",
    "| **2. Max LSWI** | Sentinel-2 | **Cloud Mask:** Same (`MSK_CLDPRB > 20`). <br>**Calc:** `(NIR-SWIR1)/(NIR+SWIR1)`. **Reducer:** `.max()`. |\n",
    "| **3. Max Backscatter** | Sentinel-1 | **Filter:** IW, VH, Descending. <br>**Process:** Apply Refined Lee Speckle Filter (5x5). <br>**Reducer:** `.max()`. |\n",
    "| **4. Integrated Backscatter** | Sentinel-1 | **Calc:** `.sum()` of VH backscatter over season (Area under curve). |\n",
    "| **5. Integrated FAPAR** | MODIS | **Product:** `MODIS/061/MCD15A3H`. **Band:** `Fpar`. <br>**Window:** `.sum()` over **`peak_start` to `peak_end`**. |\n",
    "| **6. Condition Variability** | Sentinel-2 | **Spatial CV:** Calculate `(Standard Deviation / Mean)` of the 'Max NDVI' image *spatially* within each IU polygon. |\n",
    "| **7. Rainy Days** | CHIRPS | **Product:** `UCSB-CHG/CHIRPS/DAILY`. <br>**Calc:** Count days where `precipitation > 2.5mm`. |\n",
    "| **8. Adjusted Rainfall** | CHIRPS | **Step A:** Calc Current Year Total Rain. <br>**Step B:** Calc `Normal` (10-yr avg) for same dates. <br>**Step C:** If `Current` > (1.5 * `Normal`), cap at (1.5 * `Normal`). |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Algorithmic Workflow (Decoupled)\n",
    "\n",
    "### Phase 1: Batch Extraction (GEE -> Local CSV)\n",
    "**Goal:** Download raw data to disk using Client-Side Chunking to prevent timeouts.\n",
    "1.  **Preparation:** Fetch list of `Unit_ID`s from the Input Asset.\n",
    "2.  **Loop:** Iterate through every year in the User Config.\n",
    "3.  **Action:** Call `fetch_metrics(year, crop_map_asset, roi_asset)` inside a batch loop.\n",
    "    *   Break `Unit_ID`s into batches (e.g., 50 units).\n",
    "    *   For each batch:\n",
    "        *   Filter ROI.\n",
    "        *   Run `reduceRegions`.\n",
    "        *   Fetch data using `getInfo()` or `geemap.ee_to_df()`.\n",
    "        *   Append results incrementally to `outputs/raw_data/indicators_{year}.csv`.\n",
    "        *   Use `tqdm` for progress monitoring.\n",
    "\n",
    "### Phase 2: Weight Training (Local CSV -> Weights)\n",
    "**Goal:** Load historic CSVs and learn the weights using the Entropy Method.\n",
    "**Process:**\n",
    "1.  **Load:** Read CSVs for **Training Years Only** and concatenate into `df_historic`.\n",
    "2.  **Group:** Group by `Strata_ID`.\n",
    "3.  **Calculations (Per Strata):**\n",
    "    * **Step A: Normalization ($x_{ij}$)**\n",
    "        * For Positive Indicators (NDVI, LSWI, Backscatter, FAPAR, Rain):\n",
    "        $$ x_{ij} = \\frac{X_{ij} - \\min(X_j)}{\\max(X_j) - \\min(X_j)} $$\n",
    "        * For Negative Indicators (Variability):\n",
    "        $$ x_{ij} = \\frac{\\max(X_j) - X_{ij}}{\\max(X_j) - \\min(X_j)} $$\n",
    "    * **Step B: Probability ($P_{ij}$)**\n",
    "        $$ P_{ij} = \\frac{x_{ij}}{\\sum_{i=1}^{n} x_{ij}} $$\n",
    "    * **Step C: Entropy ($E_j$)**\n",
    "        $$ E_j = -k \\sum_{i=1}^{n} (P_{ij} \\times \\ln(P_{ij})) $$\n",
    "        * Where constant $k$: $$ k = \\frac{1}{\\ln(n)} $$\n",
    "    * **Step D: Divergence ($D_j$)**\n",
    "        $$ D_j = 1 - E_j $$\n",
    "    * **Step E: Final Weight ($w_j$)**\n",
    "        $$ w_j = \\frac{D_j}{\\sum_{j=1}^{m} D_j} $$\n",
    "    * **Edge Case Handling:** If `Max(X_j) == Min(X_j)` (Zero Variance), explicitly set $w_j = 0$ to prevent division errors.\n",
    "4.  **Persist:** Save weights to `outputs/model/strata_weights.csv` and Min/Max scaling factors to `outputs/model/scaling_factors.csv`.\n",
    "\n",
    "### Phase 3: Scoring (Local CSV + Weights -> Scores)\n",
    "**Goal:** Apply weights to all years.\n",
    "1.  **Load:** Read CSVs for **All Years** (Historic + Current).\n",
    "2.  **Normalize:** Normalize the data using the **Min/Max factors saved in Phase 2**. \n",
    "    * *Crucial:* Do NOT re-calculate Min/Max on the current year data. Use the historic benchmarks.\n",
    "3.  **Calculate CHF:**\n",
    "    $$ CHF_i = \\sum_{j=1}^{m} (w_j \\times x_{ij}^{normalized}) $$\n",
    "4.  **Persist:** Save final results to `outputs/results/chf_scores_all_years.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Project Structure\n",
    "\n",
    "```text\n",
    "CHF-software/\n",
    "├── inputs/\n",
    "│   └── strata_shapefile.shp\n",
    "├── outputs/\n",
    "│   ├── raw_data/          # Phase 1 Output (Intermediate CSVs)\n",
    "│   ├── model/             # Phase 2 Output (Weights & Scaling Factors)\n",
    "│   └── results/           # Phase 3 Output (Final Scores)\n",
    "├── src/\n",
    "│   ├── __init__.py\n",
    "│   ├── gee_utils.py       # GEE Band Math & Masking\n",
    "│   ├── data_fetcher.py    # Phase 1: GEE -> CSV\n",
    "│   └── chf_engine.py      # Phase 2 & 3: CSV -> Weights -> CHF\n",
    "├── main.py                # Orchestrator\n",
    "└── requirements.txt\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
